# Gen-Image-Detector
Denoising Diffusion Model Generated Images Detection

In here, we provide a overall explanation for each folder (what they do, what external codes they use, and citations if the folder code was utilized from a different paper).


Universal-Guided-Diffusion:


deepfake:


* spoof_watermark:
This folder was uploaded from the "Supplementary Material" Section of the recentely published Feizi paper (Saberi, 2023). In this folder, the team implemented a practical implementation of the spoofing attack documented in their paper. Specifically, this code demonstrates that most of the watermarking methods are vulnerable to "spoofing attack", where a external image (not generated by the specific model) can be watermarked, which leds individuals to believe that the image was from the watermarked generative model. The code conducts these spoofing attacks by creating a nosy images, applying the specific watermark to this noisy image, and then combines it with the targted image (using image blending). This enables the so-called "clean images" to be detected as a "watermarked" image by the detector. Code also exists to calcualte the AUROC and compute its plot.



watermark_attacks:




gradient: 
This is code the team personally wrote. In this folder, we implement a new strategy using the technqiue of gradients. We attempt to train a classifier that can distguish between "real" and "fake" (image generated) using the idea of gradients. Specifically, we apply noise (in varying amounts/time steps) to each "real" and "fake" image and then denoise them. Then, we calculate the l2 norm of the difference between each pair of real and noised-denoised real image, and each pair of fake and noised-denoised fake image. A histogram of the l2 norms exists in the folder as well. 
